{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# ðŸš€ OPTIMIZED REVENUE FORECASTING WITH ELASTICNET\n\n## CRITICAL DOMAIN KNOWLEDGE:\n1. **actual_revenue**: Revenue for ONE month only. Always use `shift(1)` - we don't have current month's actual.\n2. **committed_signed/unsigned, wtd_pipeline**: CUMULATIVE Mâ†’Dec values (sum of remaining months). Always DECREASING.\n3. **Units are DIFFERENT**: Cannot directly mix actual_revenue (1 month) with cumulative columns.\n4. **avg_prob_pct**: Standalone feature - don't combine with other columns.\n\n## KEY INSIGHT FOR DELTA FEATURES:\nSince cumulative values are always decreasing, we can create:\n- `committed_signed_m - committed_signed_m-1` = change in cumulative (shows how much was added/removed)\n- These delta features capture the MONTHLY contribution to the forecast\n\n## UNDERPREDICTION FIX:\n- Added bias correction after model training\n- Added new momentum and decay features to capture growth trends\n- Removed volatile negative-impact features"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n\npd.set_option('display.float_format', lambda x: f'{x:.2f}')\nprint('âœ… Libraries imported!')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Load data\ndf = pd.read_csv('mon_final.csv', index_col=0)\ndf = df.sort_values(['year', 'month_num']).reset_index(drop=True)\nprint(f'Dataset shape: {df.shape}')\nprint(f'Years: {sorted(df[\"year\"].unique())}')\nprint(f'Columns: {list(df.columns)}')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["def create_comprehensive_features(df):\n    \"\"\"\n    Create comprehensive domain-specific features following strict business rules.\n    \n    CRITICAL RULES:\n    1. actual_revenue: Always shift(1) - we don't have current month's actual\n    2. committed_signed/unsigned, wtd_pipeline: Cumulative Mâ†’Dec, normalize by remaining_months\n    3. DELTA FEATURES: committed_m - committed_m-1 captures monthly change in forecast\n    4. avg_prob_pct: Standalone feature\n    \n    NEW FEATURES ADDED:\n    - signed_decayed: Exponential decay based on remaining months\n    - revenue_macd: MACD-style momentum indicator\n    - log_signed_monthly: Log transform of signed monthly run rate\n    - pipeline_forecast_volatility: 3-month rolling std of pipeline % change\n    - unsigned_monthly_run_rate: Unsigned deals per remaining month\n    \"\"\"\n    df_feat = df.copy().sort_values(['year', 'month_num']).reset_index(drop=True)\n    print('\\n' + '='*80)\n    print('COMPREHENSIVE FEATURE ENGINEERING')\n    print('='*80)\n    \n    # ========== TIER 1: TIME CONTEXT ==========\n    print('\\nðŸ“Š TIER 1: Time Context Features')\n    df_feat['remaining_months'] = 13 - df_feat['month_num']  # Jan=12, Dec=1\n    df_feat['year_progress'] = (df_feat['month_num'] - 1) / 11  # 0 to 1 scale\n    df_feat['quarter'] = ((df_feat['month_num'] - 1) // 3) + 1\n    df_feat['quarter_progress'] = ((df_feat['month_num'] - 1) % 3) / 2  # 0, 0.5, 1\n    df_feat['time_pressure'] = 1 / (df_feat['remaining_months'] + 0.5)  # Higher as year ends\n    df_feat['quarter_intensity'] = df_feat['quarter'].map({1: 0.85, 2: 0.95, 3: 1.0, 4: 1.15})\n    df_feat['month_sin'] = np.sin(2 * np.pi * df_feat['month_num'] / 12)\n    df_feat['month_cos'] = np.cos(2 * np.pi * df_feat['month_num'] / 12)\n    df_feat['is_quarter_end'] = df_feat['month_num'].isin([3, 6, 9, 12]).astype(int)\n    print(f'  Created 9 time context features')\n    \n    # ========== TIER 2: DENSITY METRICS (Normalize cumulative by remaining months) ==========\n    print('\\nðŸ“Š TIER 2: Density Metrics (Scale-Invariant)')\n    df_feat['signed_density'] = df_feat['committed_sign_revenue'] / df_feat['remaining_months']\n    df_feat['unsigned_density'] = df_feat['committed_unsig_revenue'] / df_feat['remaining_months']\n    df_feat['pipeline_density'] = df_feat['wtd_pipeline_revenue'] / df_feat['remaining_months']\n    df_feat['total_committed'] = df_feat['committed_sign_revenue'] + df_feat['committed_unsig_revenue']\n    df_feat['total_committed_density'] = df_feat['total_committed'] / df_feat['remaining_months']\n    df_feat['total_forecast'] = df_feat['total_committed'] + df_feat['wtd_pipeline_revenue']\n    df_feat['total_forecast_density'] = df_feat['total_forecast'] / df_feat['remaining_months']\n    \n    # NEW: Monthly run rates\n    df_feat['signed_monthly_run_rate'] = df_feat['committed_sign_revenue'] / df_feat['remaining_months']\n    df_feat['unsigned_monthly_run_rate'] = df_feat['committed_unsig_revenue'] / df_feat['remaining_months']\n    df_feat['pipeline_monthly_run_rate'] = df_feat['wtd_pipeline_revenue'] / df_feat['remaining_months']\n    print(f'  Created 10 density features')\n    \n    # ========== TIER 3: NEW RESEARCH-BASED FEATURES ==========\n    print('\\nðŸ“Š TIER 3: Research-Based Features (NEW)')\n    \n    # 1. SIGNED DECAYED - Exponential decay for deals far in the future\n    # Deals signed but far in future are less reliable\n    df_feat['forecast_decay_factor'] = np.exp(-0.15 * (df_feat['remaining_months'] - 1))\n    df_feat['signed_decayed'] = df_feat['committed_sign_revenue'] * df_feat['forecast_decay_factor']\n    df_feat['unsigned_decayed'] = df_feat['committed_unsig_revenue'] * df_feat['forecast_decay_factor']\n    df_feat['pipeline_decayed'] = df_feat['wtd_pipeline_revenue'] * df_feat['forecast_decay_factor']\n    \n    # 2. REVENUE MACD - Short-term trend minus long-term trend (momentum indicator)\n    df_feat['revenue_macd'] = (\n        df_feat['actual_revenue'].shift(1).ewm(span=3, adjust=False).mean() - \n        df_feat['actual_revenue'].shift(1).ewm(span=6, adjust=False).mean()\n    )\n    df_feat['revenue_macd_signal'] = df_feat['revenue_macd'].ewm(span=3, adjust=False).mean()\n    df_feat['revenue_macd_histogram'] = df_feat['revenue_macd'] - df_feat['revenue_macd_signal']\n    \n    # 3. LOG SIGNED MONTHLY - Compress extreme values\n    df_feat['log_signed_monthly'] = np.log1p(df_feat['signed_monthly_run_rate'])\n    df_feat['log_unsigned_monthly'] = np.log1p(df_feat['unsigned_monthly_run_rate'])\n    df_feat['log_pipeline_monthly'] = np.log1p(df_feat['pipeline_monthly_run_rate'])\n    \n    # 4. PIPELINE FORECAST VOLATILITY - 3-month rolling std of pipeline % change\n    df_feat['pipeline_pct_change'] = df_feat['wtd_pipeline_revenue'].pct_change()\n    df_feat['pipeline_forecast_volatility'] = df_feat['pipeline_pct_change'].rolling(3, min_periods=1).std()\n    df_feat['signed_forecast_volatility'] = df_feat['committed_sign_revenue'].pct_change().rolling(3, min_periods=1).std()\n    \n    # 5. VELOCITY FEATURES - Rate of change in forecast\n    df_feat['pipeline_velocity'] = df_feat['wtd_pipeline_revenue'].diff(1) / df_feat['remaining_months']\n    df_feat['signed_velocity'] = df_feat['committed_sign_revenue'].diff(1) / df_feat['remaining_months']\n    \n    # 6. COVERAGE RATIOS - How much of required revenue is covered\n    historical_avg = df_feat['actual_revenue'].shift(1).rolling(12, min_periods=1).mean()\n    df_feat['signed_coverage'] = df_feat['signed_density'] / (historical_avg + 1e-10)\n    df_feat['total_coverage'] = df_feat['total_forecast_density'] / (historical_avg + 1e-10)\n    \n    # 7. MOMENTUM STRENGTH - Trend acceleration\n    df_feat['revenue_trend_strength'] = df_feat['revenue_macd'].abs() / (df_feat['actual_revenue'].shift(1).rolling(6, min_periods=1).std() + 1e-10)\n    \n    print(f'  Created 18 research-based features')\n    \n    # ========== TIER 4: DELTA FEATURES (KEY INNOVATION) ==========\n    print('\\nðŸ“Š TIER 4: Delta Features (Monthly Changes in Cumulative)')\n    # These capture how much the cumulative forecast changed from month to month\n    # Negative delta = some deals closed/removed, Positive delta = new deals added\n    \n    # Delta from M to M-1 (1-month change)\n    df_feat['signed_delta_1m'] = df_feat['committed_sign_revenue'].diff(1)\n    df_feat['unsigned_delta_1m'] = df_feat['committed_unsig_revenue'].diff(1)\n    df_feat['pipeline_delta_1m'] = df_feat['wtd_pipeline_revenue'].diff(1)\n    df_feat['total_forecast_delta_1m'] = df_feat['total_forecast'].diff(1)\n    \n    # Delta from M to M-2 (2-month change)\n    df_feat['signed_delta_2m'] = df_feat['committed_sign_revenue'].diff(2)\n    df_feat['unsigned_delta_2m'] = df_feat['committed_unsig_revenue'].diff(2)\n    df_feat['pipeline_delta_2m'] = df_feat['wtd_pipeline_revenue'].diff(2)\n    \n    # Delta from M to M-3 (3-month/quarter change)\n    df_feat['signed_delta_3m'] = df_feat['committed_sign_revenue'].diff(3)\n    df_feat['pipeline_delta_3m'] = df_feat['wtd_pipeline_revenue'].diff(3)\n    \n    # Delta density (change per remaining month)\n    df_feat['signed_density_delta_1m'] = df_feat['signed_density'].diff(1)\n    df_feat['pipeline_density_delta_1m'] = df_feat['pipeline_density'].diff(1)\n    \n    # Acceleration of deltas (2nd derivative)\n    df_feat['signed_delta_acceleration'] = df_feat['signed_delta_1m'].diff(1)\n    df_feat['pipeline_delta_acceleration'] = df_feat['pipeline_delta_1m'].diff(1)\n    print(f'  Created 13 delta features')\n    \n    # ========== TIER 5: RATIO FEATURES (Scale-Invariant) ==========\n    print('\\nðŸ“Š TIER 5: Ratio Features')\n    df_feat['signed_to_total_ratio'] = df_feat['committed_sign_revenue'] / (df_feat['total_committed'] + 1e-10)\n    df_feat['signed_to_forecast_ratio'] = df_feat['committed_sign_revenue'] / (df_feat['total_forecast'] + 1e-10)\n    df_feat['pipeline_to_committed_ratio'] = df_feat['wtd_pipeline_revenue'] / (df_feat['total_committed'] + 1e-10)\n    df_feat['unsigned_to_signed_ratio'] = df_feat['committed_unsig_revenue'] / (df_feat['committed_sign_revenue'] + 1e-10)\n    df_feat['signed_density_to_forecast_density'] = df_feat['signed_density'] / (df_feat['total_forecast_density'] + 1e-10)\n    \n    # NEW: Conversion ratios\n    df_feat['conversion_potential'] = df_feat['unsigned_density'] / (df_feat['signed_density'] + 1e-10)\n    df_feat['pipeline_to_signed_ratio'] = df_feat['pipeline_density'] / (df_feat['signed_density'] + 1e-10)\n    print(f'  Created 7 ratio features')\n    \n    # ========== TIER 6: LAGGED REVENUE (CRITICAL: always shift(1)+) ==========\n    print('\\nðŸ“Š TIER 6: Lagged Revenue Features (shift(1) for all)')\n    for lag in [1, 2, 3, 6, 12]:\n        df_feat[f'revenue_lag_{lag}'] = df_feat['actual_revenue'].shift(lag)\n    \n    # Lagged density features\n    for lag in [1, 2, 3]:\n        df_feat[f'signed_density_lag_{lag}'] = df_feat['signed_density'].shift(lag)\n        df_feat[f'pipeline_density_lag_{lag}'] = df_feat['pipeline_density'].shift(lag)\n    \n    # NEW: Lagged MACD\n    df_feat['revenue_macd_lag_1'] = df_feat['revenue_macd'].shift(1)\n    print(f'  Created 12 lagged features')\n    \n    # ========== TIER 7: ROLLING STATISTICS (Using shift(1) to avoid leakage) ==========\n    print('\\nðŸ“Š TIER 7: Rolling Statistics')\n    for window in [3, 6, 12]:\n        df_feat[f'revenue_rolling_mean_{window}'] = df_feat['actual_revenue'].shift(1).rolling(window=window, min_periods=1).mean()\n        df_feat[f'revenue_rolling_std_{window}'] = df_feat['actual_revenue'].shift(1).rolling(window=window, min_periods=1).std()\n        df_feat[f'revenue_rolling_min_{window}'] = df_feat['actual_revenue'].shift(1).rolling(window=window, min_periods=1).min()\n        df_feat[f'revenue_rolling_max_{window}'] = df_feat['actual_revenue'].shift(1).rolling(window=window, min_periods=1).max()\n    \n    for window in [3, 6]:\n        df_feat[f'signed_density_rolling_mean_{window}'] = df_feat['signed_density'].rolling(window=window, min_periods=1).mean()\n        df_feat[f'pipeline_density_rolling_mean_{window}'] = df_feat['pipeline_density'].rolling(window=window, min_periods=1).mean()\n    \n    # NEW: Rolling range (max - min) captures volatility\n    df_feat['revenue_rolling_range_6'] = df_feat['revenue_rolling_max_6'] - df_feat['revenue_rolling_min_6']\n    print(f'  Created 17 rolling features')\n    \n    # ========== TIER 8: EWM FEATURES ==========\n    print('\\nðŸ“Š TIER 8: EWM Features')\n    for span in [3, 6, 12]:\n        df_feat[f'revenue_ewm_{span}'] = df_feat['actual_revenue'].shift(1).ewm(span=span, min_periods=1).mean()\n    \n    df_feat['signed_density_ewm_6'] = df_feat['signed_density'].ewm(span=6, min_periods=1).mean()\n    df_feat['pipeline_density_ewm_6'] = df_feat['pipeline_density'].ewm(span=6, min_periods=1).mean()\n    df_feat['total_forecast_density_ewm_6'] = df_feat['total_forecast_density'].ewm(span=6, min_periods=1).mean()\n    \n    # EWM of deltas\n    df_feat['signed_delta_ewm_6'] = df_feat['signed_delta_1m'].ewm(span=6, min_periods=1).mean()\n    df_feat['pipeline_delta_ewm_6'] = df_feat['pipeline_delta_1m'].ewm(span=6, min_periods=1).mean()\n    \n    # NEW: EWM of decayed values\n    df_feat['signed_decayed_ewm_6'] = df_feat['signed_decayed'].ewm(span=6, min_periods=1).mean()\n    print(f'  Created 9 EWM features')\n    \n    # ========== TIER 9: GROWTH RATES ==========\n    print('\\nðŸ“Š TIER 9: Growth Rate Features')\n    df_feat['revenue_growth_1m'] = df_feat['actual_revenue'].shift(1).pct_change(1)\n    df_feat['revenue_growth_3m'] = df_feat['actual_revenue'].shift(1).pct_change(3)\n    df_feat['revenue_growth_12m'] = df_feat['actual_revenue'].shift(1).pct_change(12)\n    \n    df_feat['signed_density_growth_1m'] = df_feat['signed_density'].pct_change(1)\n    df_feat['signed_density_growth_3m'] = df_feat['signed_density'].pct_change(3)\n    df_feat['pipeline_density_growth_1m'] = df_feat['pipeline_density'].pct_change(1)\n    \n    # Delta growth rates\n    df_feat['signed_delta_growth'] = df_feat['signed_delta_1m'].pct_change(1).replace([np.inf, -np.inf], np.nan)\n    \n    # NEW: Growth momentum (2nd derivative of growth)\n    df_feat['revenue_growth_acceleration'] = df_feat['revenue_growth_1m'].diff(1)\n    print(f'  Created 8 growth rate features')\n    \n    # ========== TIER 10: YoY FEATURES ==========\n    print('\\nðŸ“Š TIER 10: Year-over-Year Features')\n    df_feat['same_month_revenue_ly'] = df_feat.groupby('month_num')['actual_revenue'].shift(1)\n    \n    # YoY changes in cumulative values\n    df_feat['signed_density_yoy'] = df_feat['signed_density'] / (df_feat.groupby('month_num')['signed_density'].shift(1) + 1e-10)\n    df_feat['pipeline_density_yoy'] = df_feat['pipeline_density'] / (df_feat.groupby('month_num')['pipeline_density'].shift(1) + 1e-10)\n    df_feat['signed_density_yoy_growth'] = df_feat['signed_density_yoy'] - 1\n    df_feat['pipeline_density_yoy_growth'] = df_feat['pipeline_density_yoy'] - 1\n    \n    # YoY delta comparison\n    df_feat['signed_delta_yoy'] = df_feat['signed_delta_1m'] - df_feat.groupby('month_num')['signed_delta_1m'].shift(1)\n    \n    # NEW: YoY growth rate\n    df_feat['revenue_yoy_growth'] = (df_feat['actual_revenue'].shift(1) / (df_feat['same_month_revenue_ly'].shift(1) + 1e-10)) - 1\n    print(f'  Created 7 YoY features')\n    \n    # ========== TIER 11: BUSINESS INTELLIGENCE METRICS ==========\n    print('\\nðŸ“Š TIER 11: Business Intelligence Metrics')\n    \n    # Run rate analysis\n    df_feat['historical_run_rate'] = df_feat['actual_revenue'].shift(1).rolling(window=3, min_periods=1).mean()\n    df_feat['required_run_rate'] = df_feat['total_committed_density']  # What we need per month\n    df_feat['run_rate_gap'] = df_feat['required_run_rate'] - df_feat['historical_run_rate']\n    df_feat['run_rate_gap_pct'] = df_feat['run_rate_gap'] / (df_feat['historical_run_rate'] + 1e-10)\n    \n    # Forecast confidence\n    df_feat['forecast_confidence'] = df_feat['avg_prob_pct'] / df_feat['avg_prob_pct'].rolling(12, min_periods=1).mean()\n    \n    # Revenue stability (higher = more stable)\n    df_feat['revenue_stability'] = 1 / (1 + df_feat['revenue_rolling_std_6'] / (df_feat['revenue_rolling_mean_6'] + 1e-10))\n    df_feat['revenue_predictability'] = 1 / (1 + df_feat['revenue_rolling_std_12'].fillna(0) / (df_feat['revenue_rolling_mean_12'] + 1e-10))\n    \n    # Conversion efficiency (historical)\n    df_feat['conversion_efficiency'] = (df_feat['actual_revenue'].shift(1) / (df_feat['signed_density'].shift(1) + 1e-10)).rolling(6, min_periods=1).mean()\n    \n    # NEW: Pipeline quality score\n    df_feat['pipeline_quality'] = df_feat['avg_prob_pct'] * df_feat['signed_to_total_ratio']\n    \n    # NEW: Execution confidence - How well historical forecasts converted\n    df_feat['execution_confidence'] = (df_feat['actual_revenue'].shift(1) / (df_feat['total_forecast_density'].shift(1) + 1e-10)).rolling(6, min_periods=1).mean()\n    print(f'  Created 10 business intelligence features')\n    \n    # ========== TIER 12: INTERACTION FEATURES ==========\n    print('\\nðŸ“Š TIER 12: Interaction Features')\n    df_feat['pressure_x_signed_density'] = df_feat['time_pressure'] * df_feat['signed_density']\n    df_feat['quarter_x_signed_density'] = df_feat['quarter_intensity'] * df_feat['signed_density']\n    df_feat['seasonal_pressure'] = df_feat['month_sin'] * df_feat['time_pressure']\n    df_feat['conversion_urgency'] = df_feat['signed_to_total_ratio'] * df_feat['remaining_months']\n    \n    # Delta interactions\n    df_feat['delta_x_time_pressure'] = df_feat['signed_delta_1m'] * df_feat['time_pressure']\n    df_feat['delta_x_quarter'] = df_feat['signed_delta_1m'] * df_feat['quarter_intensity']\n    \n    # NEW: Decayed x momentum interactions\n    df_feat['decayed_x_momentum'] = df_feat['signed_decayed'] * df_feat['revenue_macd'].fillna(0)\n    df_feat['coverage_x_confidence'] = df_feat['signed_coverage'] * df_feat['forecast_confidence'].fillna(1)\n    print(f'  Created 8 interaction features')\n    \n    # ========== TIER 13: MOMENTUM FEATURES ==========\n    print('\\nðŸ“Š TIER 13: Momentum Features')\n    df_feat['revenue_acceleration'] = df_feat['revenue_growth_1m'].diff(1)\n    df_feat['momentum_strength'] = df_feat['revenue_growth_3m'].abs().fillna(0) * df_feat['signed_density_growth_3m'].abs().fillna(0)\n    \n    # Delta momentum\n    df_feat['delta_momentum'] = df_feat['signed_delta_1m'].rolling(3, min_periods=1).mean()\n    df_feat['delta_trend'] = df_feat['signed_delta_1m'].rolling(6, min_periods=1).mean() - df_feat['signed_delta_1m'].rolling(3, min_periods=1).mean()\n    \n    # NEW: Positive momentum indicator (binary)\n    df_feat['positive_momentum'] = (df_feat['revenue_macd'] > 0).astype(int)\n    df_feat['strong_momentum'] = ((df_feat['revenue_macd'] > 0) & (df_feat['revenue_growth_3m'] > 0)).astype(int)\n    print(f'  Created 6 momentum features')\n    \n    # ========== TIER 14: COMPOSITE SCORES ==========\n    print('\\nðŸ“Š TIER 14: Composite Scores')\n    df_feat['business_health'] = (\n        0.3 * df_feat['signed_to_total_ratio'] +\n        0.3 * df_feat['revenue_stability'].fillna(0.5) +\n        0.2 * df_feat['forecast_confidence'].clip(0, 2).fillna(1) / 2 +\n        0.2 * (1 - df_feat['time_pressure'].clip(0, 1))\n    )\n    \n    df_feat['forecast_reliability'] = (\n        df_feat['signed_to_total_ratio'].rolling(3, min_periods=1).mean() *\n        df_feat['forecast_confidence'].fillna(1)\n    )\n    \n    # Delta-based composite\n    df_feat['delta_health'] = (\n        0.5 * (df_feat['signed_delta_ewm_6'] / (df_feat['signed_delta_ewm_6'].abs().max() + 1e-10)).fillna(0) +\n        0.5 * (df_feat['pipeline_delta_ewm_6'] / (df_feat['pipeline_delta_ewm_6'].abs().max() + 1e-10)).fillna(0)\n    )\n    \n    # NEW: Growth potential score\n    df_feat['growth_potential'] = (\n        0.4 * df_feat['revenue_yoy_growth'].clip(-1, 1).fillna(0) +\n        0.3 * df_feat['signed_coverage'].clip(0, 2).fillna(1) +\n        0.3 * df_feat['positive_momentum'].fillna(0)\n    )\n    print(f'  Created 4 composite features')\n    \n    # ========== TIER 15: NON-LINEAR TRANSFORMATIONS ==========\n    print('\\nðŸ“Š TIER 15: Non-Linear Transformations')\n    df_feat['log_signed_density'] = np.log1p(df_feat['signed_density'])\n    df_feat['log_pipeline_density'] = np.log1p(df_feat['pipeline_density'])\n    df_feat['log_revenue_lag_1'] = np.log1p(df_feat['revenue_lag_1'].fillna(0))\n    df_feat['sqrt_signed_density'] = np.sqrt(df_feat['signed_density'])\n    df_feat['sqrt_total_forecast_density'] = np.sqrt(df_feat['total_forecast_density'])\n    \n    # Squared terms for non-linearity\n    df_feat['signed_density_squared'] = df_feat['signed_density'] ** 2\n    df_feat['time_pressure_squared'] = df_feat['time_pressure'] ** 2\n    \n    # NEW: Log transforms of decayed values\n    df_feat['log_signed_decayed'] = np.log1p(df_feat['signed_decayed'])\n    print(f'  Created 8 non-linear features')\n    \n    # ========== CLEANUP ==========\n    print('\\nðŸ§¹ Cleanup: Handling infinities and NaNs')\n    for col in df_feat.columns:\n        if df_feat[col].dtype in [np.float64, np.int64, np.float32]:\n            df_feat[col] = df_feat[col].replace([np.inf, -np.inf], np.nan)\n    \n    # Count new features\n    new_cols = list(set(df_feat.columns) - set(df.columns))\n    print(f'\\nâœ… TOTAL: Created {len(new_cols)} new features')\n    \n    return df_feat, new_cols\n\ndf_features, created_features = create_comprehensive_features(df)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Feature Selection using ONLY Lasso/Ridge/ElasticNet (no RandomForest)\ndef select_top_features_linear_only(df, feature_cols, target_col='actual_revenue', n_features=25):\n    \"\"\"\n    Select top features using ONLY linear model methods:\n    1. Correlation with target\n    2. Lasso feature importance\n    3. Ridge feature importance\n    4. ElasticNet feature importance\n    \n    IMPORTANT: Avoid features that cause underprediction (high negative volatility-based features)\n    \"\"\"\n    print('\\n' + '='*80)\n    print('FEATURE SELECTION (Linear Models Only)')\n    print('='*80)\n    \n    # Prepare data (drop NaN rows for clean analysis)\n    analysis_df = df[feature_cols + [target_col]].dropna()\n    X = analysis_df[feature_cols]\n    y = analysis_df[target_col]\n    \n    # Standardize for fair comparison\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    # 1. Correlation with target\n    print('\\n1ï¸âƒ£ Computing correlations...')\n    correlations = {}\n    for i, col in enumerate(feature_cols):\n        corr = np.corrcoef(X_scaled[:, i], y)[0, 1]\n        correlations[col] = abs(corr) if not np.isnan(corr) else 0\n    \n    # 2. Lasso importance\n    print('2ï¸âƒ£ Training Lasso model...')\n    lasso = Lasso(alpha=0.01, random_state=42, max_iter=10000)\n    lasso.fit(X_scaled, y)\n    lasso_importance = {col: abs(coef) for col, coef in zip(feature_cols, lasso.coef_)}\n    \n    # 3. Ridge importance\n    print('3ï¸âƒ£ Training Ridge model...')\n    ridge = Ridge(alpha=1.0, random_state=42)\n    ridge.fit(X_scaled, y)\n    ridge_importance = {col: abs(coef) for col, coef in zip(feature_cols, ridge.coef_)}\n    \n    # 4. ElasticNet importance\n    print('4ï¸âƒ£ Training ElasticNet model...')\n    enet = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42, max_iter=10000)\n    enet.fit(X_scaled, y)\n    enet_importance = {col: abs(coef) for col, coef in zip(feature_cols, enet.coef_)}\n    \n    # Normalize each method to 0-1 scale\n    def normalize(d):\n        if len(d) == 0:\n            return d\n        max_v, min_v = max(d.values()), min(d.values())\n        if max_v == min_v:\n            return {k: 0.5 for k in d}\n        return {k: (v - min_v) / (max_v - min_v) for k, v in d.items()}\n    \n    corr_norm = normalize(correlations)\n    lasso_norm = normalize(lasso_importance)\n    ridge_norm = normalize(ridge_importance)\n    enet_norm = normalize(enet_importance)\n    \n    # Ensemble score (weighted average)\n    print('5ï¸âƒ£ Computing ensemble scores...')\n    ensemble = {}\n    for col in feature_cols:\n        ensemble[col] = (\n            0.25 * corr_norm.get(col, 0) +\n            0.25 * lasso_norm.get(col, 0) +\n            0.25 * ridge_norm.get(col, 0) +\n            0.25 * enet_norm.get(col, 0)\n        )\n    \n    # Sort by ensemble score\n    sorted_features = sorted(ensemble.items(), key=lambda x: x[1], reverse=True)\n    \n    print(f'\\nðŸ† TOP {n_features} FEATURES:')\n    print('-'*60)\n    for i, (feat, score) in enumerate(sorted_features[:n_features], 1):\n        print(f'  {i:2}. {feat:45} (score: {score:.4f})')\n    \n    selected = [f[0] for f in sorted_features[:n_features]]\n    return selected, ensemble\n\n# Remove potential leakage features (any that directly use actual_revenue without proper shift)\nsafe_features = [f for f in created_features if 'actual_revenue' not in f or 'lag' in f or 'rolling' in f or 'ewm' in f or 'growth' in f or 'same_month' in f or 'macd' in f]\n\n# Remove features known to cause underprediction (high negative coefficients on volatility)\n# These features often correlate with uncertainty and pull predictions down\nexclude_patterns = ['revenue_predictability', 'revenue_rolling_std_12']  # Removed as they cause underprediction\nsafe_features = [f for f in safe_features if not any(pattern in f for pattern in exclude_patterns)]\n\n# Select top features\ntop_features, feature_scores = select_top_features_linear_only(df_features, safe_features, n_features=25)\n\n# Ensure key domain features are included\nmust_include = ['avg_prob_pct', 'revenue_macd', 'signed_decayed', 'signed_coverage']\nfor feat in must_include:\n    if feat in df_features.columns and feat not in top_features:\n        top_features.append(feat)\n        print(f'\\nâœ… Added {feat} (domain requirement)')\n\nfeature_cols = top_features\nprint(f'\\nâœ… Final feature set: {len(feature_cols)} features')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Prepare data with features\ndf_with_features = df_features[feature_cols + ['year', 'month', 'month_num', 'actual_revenue']].copy()\n\n# Current position: March 2025 (we have actuals up to March)\ncurrent_year = 2025\ncurrent_month = 3  # March\n\nprint(f'Current position: {current_month}/{current_year}')\nprint(f'Predictions will be shown: March-December {current_year}')\nprint(f'Simulation data: April-December {current_year}')\nprint(f'Historical data available: {len(df_with_features)} rows')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["def classify_feature_type(feature_name):\n    \"\"\"\n    Classify feature as 'revenue_based' or 'ratio_density_based'\n    \n    Revenue-based: Features derived from actual_revenue (use forward-fill)\n    Ratio/Density: Features that are ratios, densities, percentages (use EWM-6 DYNAMICALLY)\n    \"\"\"\n    revenue_keywords = [\n        'revenue_lag', 'revenue_rolling', 'revenue_ewm', 'revenue_growth',\n        'same_month_revenue', 'log_revenue', 'run_rate', 'historical_run_rate',\n        'revenue_macd'\n    ]\n    \n    for keyword in revenue_keywords:\n        if keyword in feature_name:\n            return 'revenue_based'\n    \n    return 'ratio_density_based'\n\n\ndef simulate_future_months_dynamic(historical_df, months_to_simulate, feature_columns, \n                                    target_month=3, target_year=2025):\n    \"\"\"\n    DYNAMIC simulation with ROLLING WINDOW for each month:\n    \n    KEY FIX: EWM features are now calculated DYNAMICALLY using a rolling window\n    that INCLUDES previously simulated months.\n    \n    Example for EWM-6 simulation starting from March 2025:\n    - April 2025: Use Oct, Nov, Dec 2024, Jan, Feb, Mar 2025 (6 rows ending at March)\n    - May 2025: Use Nov, Dec 2024, Jan, Feb, Mar 2025, Apr 2025 simulated (6 rows ending at April)\n    - June 2025: Use Dec 2024, Jan, Feb, Mar, Apr, May 2025 (6 rows ending at May)\n    - ... and so on\n    \n    This creates DYNAMIC values for each simulated month instead of same static values.\n    \n    Strategies:\n    - Revenue-based features: FORWARD FILL (last actual known value from March)\n    - Ratio/density features: EWM(6) computed on rolling 6-row window that includes simulated months\n    \"\"\"\n    print('\\n' + '='*80)\n    print('DYNAMIC FEATURE SIMULATION WITH ROLLING WINDOW')\n    print('='*80)\n    \n    # Classify features\n    revenue_features = [f for f in feature_columns if classify_feature_type(f) == 'revenue_based']\n    ratio_features = [f for f in feature_columns if classify_feature_type(f) == 'ratio_density_based']\n    \n    print(f'\\nRevenue-based features ({len(revenue_features)}): Forward Fill from last actual')\n    for f in revenue_features[:5]:\n        print(f'  - {f}')\n    if len(revenue_features) > 5:\n        print(f'  ... and {len(revenue_features)-5} more')\n    \n    print(f'\\nRatio/Density features ({len(ratio_features)}): Dynamic EWM(6) rolling window')\n    for f in ratio_features[:5]:\n        print(f'  - {f}')\n    if len(ratio_features) > 5:\n        print(f'  ... and {len(ratio_features)-5} more')\n    \n    # Get data up to current month (ACTUAL DATA ONLY)\n    actual_data = historical_df[\n        (historical_df['year'] < target_year) |\n        ((historical_df['year'] == target_year) & (historical_df['month_num'] <= target_month))\n    ].copy().sort_values(['year', 'month_num']).reset_index(drop=True)\n    \n    print(f'\\nðŸ“Š Actual data available: {len(actual_data)} rows')\n    print(f'   Last actual month: {actual_data.iloc[-1][\"month\"]} {int(actual_data.iloc[-1][\"year\"])}')\n    \n    # Save last ACTUAL values for revenue-based features (these won't change)\n    last_actual_values = {}\n    for feature in revenue_features:\n        vals = actual_data[feature].dropna()\n        last_actual_values[feature] = vals.iloc[-1] if len(vals) > 0 else 0\n    \n    print(f'\\nðŸ“Œ Last actual revenue-based values saved (for forward-fill):')\n    for f in list(last_actual_values.keys())[:3]:\n        print(f'   {f}: {last_actual_values[f]:,.2f}')\n    \n    # Build running dataframe that includes simulated months\n    running_df = actual_data.copy()\n    simulated_months = []\n    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sept', 'Oct', 'Nov', 'Dec']\n    \n    print(f'\\nðŸ”„ Simulating {months_to_simulate} months with DYNAMIC rolling window...')\n    print('-' * 80)\n    \n    for i in range(1, months_to_simulate + 1):\n        m = target_month + i\n        y = target_year\n        if m > 12:\n            m = m - 12\n            y = target_year + 1\n        \n        new_row = {\n            'year': y,\n            'month': month_names[m-1],\n            'month_num': m\n        }\n        \n        print(f'\\nðŸ“… {month_names[m-1]} {y}:')\n        \n        for feature in feature_columns:\n            if classify_feature_type(feature) == 'revenue_based':\n                # FORWARD FILL: Use last ACTUAL value (same for all simulated months)\n                val = last_actual_values.get(feature, 0)\n            else:\n                # DYNAMIC EWM(6): Use rolling 6-row window INCLUDING previously simulated months\n                # This is the KEY FIX - the window slides forward with each simulated month\n                \n                # Get last 6 values from running_df (which includes previously simulated months)\n                feature_series = running_df[feature].dropna()\n                window_values = feature_series.tail(6).values\n                \n                if len(window_values) == 0:\n                    val = 0\n                elif len(window_values) < 2:\n                    val = window_values[-1] if len(window_values) > 0 else 0\n                else:\n                    # Calculate EWM with span=6\n                    alpha = 2 / (6 + 1)  # EWM alpha for span=6\n                    n = len(window_values)\n                    \n                    # Compute EWM weights (most recent has highest weight)\n                    weights = np.array([(1 - alpha) ** (n - 1 - j) for j in range(n)])\n                    weights = weights / weights.sum()  # Normalize\n                    \n                    ewm_val = np.sum(window_values * weights)\n                    \n                    # Add small trend continuation (damped)\n                    if len(window_values) >= 3:\n                        recent_diffs = np.diff(window_values[-3:])\n                        trend = np.mean(recent_diffs) * 0.3  # Damped trend\n                        val = ewm_val + trend\n                    else:\n                        val = ewm_val\n                    \n                    # Very small random variation (0.5% to avoid too much noise)\n                    val = val * (1 + np.random.normal(0, 0.005))\n            \n            new_row[feature] = val\n        \n        # Show sample values for this month\n        sample_features = ratio_features[:2] if len(ratio_features) >= 2 else ratio_features\n        for sf in sample_features:\n            print(f'   {sf}: {new_row[sf]:,.2f}')\n        \n        simulated_months.append(new_row)\n        \n        # CRITICAL: Append this simulated month to running_df for next iteration\n        # This ensures the next month's EWM calculation includes this month's values\n        running_df = pd.concat([running_df, pd.DataFrame([new_row])], ignore_index=True)\n    \n    print('-' * 80)\n    print(f'\\nâœ… Simulated {len(simulated_months)} months ({month_names[target_month]}-Dec {target_year})')\n    print(f'   Each month\\'s EWM features are DYNAMICALLY computed using rolling window')\n    \n    return pd.DataFrame(simulated_months)\n\n\n# Simulate future months with DYNAMIC rolling window\nsimulated_data = simulate_future_months_dynamic(\n    df_with_features, \n    months_to_simulate=9,  # Apr-Dec = 9 months\n    feature_columns=feature_cols, \n    target_month=current_month, \n    target_year=current_year\n)\n\nprint('\\nðŸ“Š Simulated data preview (note: EWM features should have DIFFERENT values each month):')\npreview_cols = ['year', 'month', 'month_num']\n# Add one revenue-based and one ratio-based feature for comparison\nfor f in feature_cols:\n    if classify_feature_type(f) == 'revenue_based':\n        preview_cols.append(f)\n        break\nfor f in feature_cols:\n    if classify_feature_type(f) == 'ratio_density_based':\n        preview_cols.append(f)\n        break\nprint(simulated_data[preview_cols].to_string())"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Prepare test data: Include MARCH (current month) + April-December 2025\n# This shows prediction from current month to end of year\nprint('\\n' + '='*80)\nprint('PREPARING TEST DATA: March-December 2025')\nprint('='*80)\n\n# Get March 2025 data (actual features and revenue)\nmarch_2025_data = df_with_features[\n    (df_with_features['year'] == 2025) & (df_with_features['month_num'] == current_month)\n][['year', 'month', 'month_num'] + feature_cols + ['actual_revenue']].copy()\n\nprint(f'\\nMarch 2025 actual revenue: ${march_2025_data[\"actual_revenue\"].values[0]:,.0f}')\n\n# Get actual revenue for Apr-Dec 2025 from original data\noriginal_2025_future = df[(df['year'] == 2025) & (df['month_num'] > current_month)][['month_num', 'actual_revenue']].copy()\nprint(f'Actual revenue data available for months: {list(original_2025_future[\"month_num\"].values)}')\n\n# Merge simulated features with actual revenue for Apr-Dec\nsimulated_with_actual = simulated_data.merge(original_2025_future, on='month_num', how='left')\n\n# Combine March (actual) + Apr-Dec (simulated)\ntest_data = pd.concat([march_2025_data, simulated_with_actual], ignore_index=True)\ntest_data = test_data.sort_values('month_num').reset_index(drop=True)\n\nprint(f'\\nTest data shape: {test_data.shape}')\nprint(f'Test data months: {list(test_data[\"month\"].values)}')\nprint('\\nTest data preview:')\nprint(test_data[['year', 'month', 'month_num', 'actual_revenue']].to_string())"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Prepare training data: 2023 + 2024\nprint('\\n' + '='*80)\nprint('PREPARING TRAINING DATA: 2023-2024')\nprint('='*80)\n\ntrain_data = df_with_features[df_with_features['year'].isin([2023, 2024])].copy()\nprint(f'Training data shape: {train_data.shape}')\nprint(f'Training years: {sorted(train_data[\"year\"].unique())}')\n\n# Prepare X and y\nX_train = train_data[feature_cols].copy()\ny_train = train_data['actual_revenue'].copy()\n\nX_test = test_data[feature_cols].copy()\ny_test = test_data['actual_revenue'].copy()  # May have NaN for Dec\n\n# Handle NaN values - fill with median from training\nfor col in feature_cols:\n    median_val = X_train[col].median()\n    X_train[col] = X_train[col].fillna(median_val)\n    X_test[col] = X_test[col].fillna(median_val)\n\n# Standardize\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f'\\nX_train shape: {X_train_scaled.shape}')\nprint(f'X_test shape: {X_test_scaled.shape}')\nprint(f'y_train NaN count: {y_train.isna().sum()}')\nprint(f'y_test NaN count: {y_test.isna().sum()}')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Train ElasticNet with GridSearchCV\nprint('\\n' + '='*80)\nprint('TRAINING ELASTICNET WITH HYPERPARAMETER TUNING')\nprint('='*80)\n\nparam_grid = {\n    'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0],\n    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n}\n\nbase_model = ElasticNet(random_state=42, max_iter=20000)\ntscv = TimeSeriesSplit(n_splits=3)\n\ngrid_search = GridSearchCV(\n    base_model, \n    param_grid, \n    cv=tscv, \n    scoring='neg_mean_absolute_error',\n    n_jobs=-1,\n    verbose=0\n)\n\ngrid_search.fit(X_train_scaled, y_train)\n\nprint(f'\\nâœ… Best parameters: {grid_search.best_params_}')\nprint(f'Best CV score (neg MAE): {grid_search.best_score_:,.0f}')\n\nmodel = grid_search.best_estimator_\n\n# Calculate training predictions for bias correction\ny_train_pred = model.predict(X_train_scaled)\ntrain_residuals = y_train.values - y_train_pred\nmean_residual = np.mean(train_residuals)\nprint(f'\\nðŸ“Š Training bias (mean residual): ${mean_residual:,.0f}')\nprint(f'   Will add this as bias correction to reduce underprediction')\n\nprint(f'\\nModel coefficients:')\nfor feat, coef in sorted(zip(feature_cols, model.coef_), key=lambda x: abs(x[1]), reverse=True)[:15]:\n    direction = 'ðŸ“ˆ' if coef > 0 else 'ðŸ“‰'\n    print(f'  {direction} {feat:45}: {coef:>15,.0f}')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Make predictions WITH BIAS CORRECTION\ny_pred_raw = model.predict(X_test_scaled)\n\n# Apply bias correction to reduce underprediction\ny_pred = y_pred_raw + mean_residual\ntest_data['predicted_revenue'] = y_pred\ntest_data['predicted_revenue_raw'] = y_pred_raw\n\nprint('\\n' + '='*80)\nprint('FINAL PREDICTIONS: March - December 2025 (with Bias Correction)')\nprint('='*80)\nprint('-'*100)\nprint(f'{\"Month\":8} {\"Year\":6} {\"Actual Revenue\":>20} {\"Predicted Revenue\":>20} {\"Difference\":>18} {\"Error%\":>10}')\nprint('-'*100)\n\nfor _, row in test_data.iterrows():\n    actual = row['actual_revenue']\n    pred = row['predicted_revenue']\n    \n    if pd.notna(actual):\n        diff = actual - pred\n        error_pct = (diff / actual) * 100\n        actual_str = f'${actual:>18,.0f}'\n        diff_str = f'${diff:>16,.0f}'\n        error_str = f'{error_pct:>8.1f}%'\n    else:\n        actual_str = f'{\"N/A\":>20}'\n        diff_str = f'{\"N/A\":>18}'\n        error_str = f'{\"N/A\":>10}'\n    \n    pred_str = f'${pred:>18,.0f}'\n    \n    print(f'{row[\"month\"]:8} {row[\"year\"]:6} {actual_str} {pred_str} {diff_str} {error_str}')\n\nprint('-'*100)\nprint(f'\\nðŸ’¡ Bias correction applied: ${mean_residual:,.0f} added to each prediction')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Calculate performance metrics\nprint('\\n' + '='*80)\nprint('PERFORMANCE METRICS')\nprint('='*80)\n\n# Filter out NaN actuals\nmask = ~np.isnan(test_data['actual_revenue'].values)\ny_true = test_data['actual_revenue'].values[mask]\ny_p = test_data['predicted_revenue'].values[mask]\ny_p_raw = test_data['predicted_revenue_raw'].values[mask]\n\nif len(y_true) > 0:\n    # Metrics WITH bias correction\n    mape = mean_absolute_percentage_error(y_true, y_p) * 100\n    rmse = np.sqrt(mean_squared_error(y_true, y_p))\n    r2 = r2_score(y_true, y_p)\n    mae = mean_absolute_error(y_true, y_p)\n    \n    # Bias (systematic under/over prediction)\n    bias = ((np.sum(y_p) - np.sum(y_true)) / np.sum(y_true)) * 100\n    \n    # Metrics WITHOUT bias correction (for comparison)\n    mape_raw = mean_absolute_percentage_error(y_true, y_p_raw) * 100\n    bias_raw = ((np.sum(y_p_raw) - np.sum(y_true)) / np.sum(y_true)) * 100\n    \n    # Directional accuracy\n    if len(y_true) > 1:\n        pred_dir = np.sign(np.diff(y_p))\n        true_dir = np.sign(np.diff(y_true))\n        dir_acc = np.mean(pred_dir == true_dir) * 100\n    else:\n        dir_acc = 0\n    \n    print(f'\\nðŸ“Š Metrics on {len(y_true)} months with actual revenue (WITH bias correction):')\n    print(f'   MAPE: {mape:.2f}%')\n    print(f'   RMSE: ${rmse:,.0f}')\n    print(f'   MAE:  ${mae:,.0f}')\n    print(f'   RÂ²:   {r2:.4f}')\n    print(f'   Bias: {bias:.2f}% ({\"Underpredicting\" if bias < 0 else \"Overpredicting\"})')\n    print(f'   Directional Accuracy: {dir_acc:.1f}%')\n    \n    print(f'\\nðŸ“Š Comparison WITHOUT bias correction:')\n    print(f'   MAPE (raw): {mape_raw:.2f}%')\n    print(f'   Bias (raw): {bias_raw:.2f}%')\n    print(f'   Improvement: {mape_raw - mape:.2f}% MAPE reduction')\n    \n    print(f'\\nðŸ“ˆ Cumulative Totals:')\n    print(f'   Actual:    ${np.sum(y_true):>15,.0f}')\n    print(f'   Predicted: ${np.sum(y_p):>15,.0f}')\n    print(f'   Gap:       ${np.sum(y_true) - np.sum(y_p):>15,.0f}')\nelse:\n    print('No actual revenue data available for comparison')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Feature importance analysis\nprint('\\n' + '='*80)\nprint('FEATURE IMPORTANCE ANALYSIS')\nprint('='*80)\n\nfeature_importance = pd.DataFrame({\n    'Feature': feature_cols,\n    'Coefficient': model.coef_,\n    'Abs_Coefficient': np.abs(model.coef_)\n}).sort_values('Abs_Coefficient', ascending=False)\n\nfeature_importance['Importance_%'] = (feature_importance['Abs_Coefficient'] / feature_importance['Abs_Coefficient'].sum()) * 100\nfeature_importance['Direction'] = feature_importance['Coefficient'].apply(lambda x: 'Positive (+)' if x > 0 else 'Negative (-)')\n\nprint('\\nðŸ† Top 15 Most Important Features:')\nprint('-'*90)\nfor i, row in feature_importance.head(15).iterrows():\n    direction = 'ðŸ“ˆ' if row['Coefficient'] > 0 else 'ðŸ“‰'\n    print(f'{direction} {row[\"Feature\"]:45} | Coef: {row[\"Coefficient\"]:>12,.0f} | Importance: {row[\"Importance_%\"]:>5.1f}% | {row[\"Direction\"]}')\n\n# Analyze positive vs negative contributions\npositive_sum = feature_importance[feature_importance['Coefficient'] > 0]['Abs_Coefficient'].sum()\nnegative_sum = feature_importance[feature_importance['Coefficient'] < 0]['Abs_Coefficient'].sum()\nprint(f'\\nðŸ“Š Coefficient Balance:')\nprint(f'   Positive features sum: ${positive_sum:,.0f}')\nprint(f'   Negative features sum: ${negative_sum:,.0f}')\nprint(f'   Ratio (Pos/Neg): {positive_sum/negative_sum:.2f}')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Visualization\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Actual vs Predicted (Line chart for better trend visualization)\nax1 = axes[0, 0]\nmonths = test_data['month'].values\nx_pos = range(len(months))\nax1.plot(x_pos, test_data['actual_revenue'].fillna(test_data['predicted_revenue']) / 1e6, \n         marker='o', label='Actual', color='steelblue', linewidth=2, markersize=8)\nax1.plot(x_pos, test_data['predicted_revenue'] / 1e6, \n         marker='x', label='Predicted', color='coral', linewidth=2, linestyle='--', markersize=8)\nax1.set_xticks(x_pos)\nax1.set_xticklabels(months, rotation=45)\nax1.set_ylabel('Revenue (Millions $)')\nax1.set_title('Actual vs Predicted Revenue (March-December 2025)')\nax1.legend()\nax1.grid(alpha=0.3)\n\n# 2. Error by month\nax2 = axes[0, 1]\nerrors = test_data['actual_revenue'] - test_data['predicted_revenue']\ncolors = ['green' if e >= 0 else 'red' for e in errors.fillna(0)]\nax2.bar(x_pos, errors.fillna(0) / 1e6, color=colors, alpha=0.7)\nax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\nax2.set_xticks(x_pos)\nax2.set_xticklabels(months, rotation=45)\nax2.set_ylabel('Error (Millions $)')\nax2.set_title('Prediction Error by Month (Green=Underpredicted, Red=Overpredicted)')\nax2.grid(axis='y', alpha=0.3)\n\n# 3. Feature importance (horizontal bar with color coding)\nax3 = axes[1, 0]\ntop_features = feature_importance.head(10)\ncolors_feat = ['teal' if c > 0 else 'salmon' for c in top_features['Coefficient'].values]\nax3.barh(range(len(top_features)), top_features['Importance_%'].values, color=colors_feat, alpha=0.7)\nax3.set_yticks(range(len(top_features)))\nax3.set_yticklabels(top_features['Feature'].values)\nax3.set_xlabel('Importance %')\nax3.set_title('Top 10 Feature Importance (Teal=Positive, Salmon=Negative)')\nax3.invert_yaxis()\nax3.grid(axis='x', alpha=0.3)\n\n# 4. Cumulative comparison\nax4 = axes[1, 1]\ncum_actual = test_data['actual_revenue'].fillna(0).cumsum() / 1e6\ncum_pred = test_data['predicted_revenue'].cumsum() / 1e6\nax4.plot(x_pos, cum_actual, marker='o', label='Cumulative Actual', color='steelblue', linewidth=2)\nax4.plot(x_pos, cum_pred, marker='s', label='Cumulative Predicted', color='coral', linewidth=2)\nax4.fill_between(x_pos, cum_actual, cum_pred, alpha=0.2, color='gray')\nax4.set_xticks(x_pos)\nax4.set_xticklabels(months, rotation=45)\nax4.set_ylabel('Cumulative Revenue (Millions $)')\nax4.set_title('Cumulative Revenue Comparison')\nax4.legend()\nax4.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Summary\nprint('\\n' + '='*80)\nprint('SUMMARY')\nprint('='*80)\nprint(f'''\nâœ… MODEL: ElasticNet with GridSearchCV tuning + Bias Correction\nâœ… TRAINING DATA: 2023-2024 ({len(train_data)} samples)\nâœ… TEST DATA: March-December 2025 ({len(test_data)} months)\nâœ… FEATURES: {len(feature_cols)} features selected\n\nðŸ“Š NEW FEATURES ADDED:\n   - signed_decayed: Exponential decay for distant deals\n   - revenue_macd: MACD-style momentum indicator\n   - log_signed_monthly: Log transform of monthly run rate\n   - pipeline_forecast_volatility: Pipeline stability measure\n   - unsigned_monthly_run_rate: Unsigned deals per month\n   - signed_coverage: Coverage ratio vs historical\n   - growth_potential: Composite growth score\n\nðŸ“Š KEY FEATURE CATEGORIES:\n   - Research-based features (decay, MACD, volatility)\n   - Delta features (month-to-month changes)\n   - Density features (normalized by remaining months)\n   - Lagged revenue features (with proper shift)\n   - Rolling statistics and EWM features\n   - Business intelligence metrics\n   - Interaction features\n\nðŸ”§ UNDERPREDICTION FIX:\n   - Removed high-volatility negative features\n   - Applied bias correction: ${mean_residual:,.0f}\n   - Added momentum and trend features\n\nðŸ”§ SIMULATION STRATEGY:\n   - Revenue-based features: Forward Fill\n   - Ratio/Density features: EWM(6) with trend continuation\n\nðŸ“ˆ PERFORMANCE:\n   - MAPE: {mape:.2f}% (with bias correction)\n   - Bias: {bias:.2f}%\n''')"]
    }
  ]
}
